{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR_cIASOtQqM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt95m3zGtMDw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1bLQRE0tRY2"
      },
      "outputs": [],
      "source": [
        "model_names = [\n",
        "    \"facebook/bart-base\",\n",
        "    \"allenai/longformer-base-4096\",\n",
        "    \"google/electra-small-discriminator\",\n",
        "    \"microsoft/mpnet-base\",\n",
        "    \"squeezebert/squeezebert-uncased\",\n",
        "    \"deepset/sentence_bert\",\n",
        "    \"vinai/phobert-base\",\n",
        "    \"bert-base-uncased\",\n",
        "    \"roberta-base\",\n",
        "    \"distilbert-base-uncased\",\n",
        "    \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZp79A9WtbEW"
      },
      "outputs": [],
      "source": [
        "# Similarity and distance metrics\n",
        "parameters = [\"cosine_similarity\", \"euclidean_distance\", \"manhattan_distance\", \"minkowski_distance\", \"correlation_coefficient\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY2-ZXaDtd4O"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# Sample paragraphs for comparison\n",
        "paragraph1 = \"\"\"\n",
        "Natural language processing (NLP) is a field of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.\n",
        "\"\"\"\n",
        "paragraph2 = \"\"\"\n",
        "Machine learning is a subset of artificial intelligence that involves the development of algorithms and statistical models that enable computers to perform specific tasks without explicit programming. In the context of natural language processing, machine learning algorithms are often used to analyze and understand the structure and meaning of human language.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLGmnvU3tjV0",
        "outputId": "a82c7c32-b64f-4923-9c20-c9a0f0cbda95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/bart-base. Creating a new one with mean pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name allenai/longformer-base-4096. Creating a new one with mean pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name google/electra-small-discriminator. Creating a new one with mean pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name microsoft/mpnet-base. Creating a new one with mean pooling.\n",
            "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name squeezebert/squeezebert-uncased. Creating a new one with mean pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name deepset/sentence_bert. Creating a new one with mean pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name vinai/phobert-base. Creating a new one with mean pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name roberta-base. Creating a new one with mean pooling.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name distilbert-base-uncased. Creating a new one with mean pooling.\n"
          ]
        }
      ],
      "source": [
        "for model_name in model_names:\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Generate sentence embeddings\n",
        "    embedding1 = model.encode([paragraph1])\n",
        "    embedding2 = model.encode([paragraph2])\n",
        "\n",
        "    # Calculate similarity and distance metrics\n",
        "    cosine_sim = cosine_similarity(embedding1, embedding2)[0][0]\n",
        "    euclidean_dist = np.linalg.norm(embedding1 - embedding2)\n",
        "    manhattan_dist = np.abs(embedding1 - embedding2).sum()\n",
        "    minkowski_dist = np.power(np.power(np.abs(embedding1 - embedding2), 3).sum(), 1/3)\n",
        "    correlation_coeff = np.corrcoef(embedding1[0], embedding2[0])[0, 1]\n",
        "\n",
        "    # Store results\n",
        "    parameter_values = [cosine_sim, euclidean_dist, manhattan_dist, minkowski_dist, correlation_coeff]\n",
        "    data.append([model_name] + parameter_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3E_2a0TtnTV"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame\n",
        "columns = [\"Model\"] + parameters\n",
        "df = pd.DataFrame(data, columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Cq3YAw6ttBG"
      },
      "outputs": [],
      "source": [
        "df_normalized = df.copy()\n",
        "for param in parameters:\n",
        "    df_normalized[param] = (df[param] - df[param].min()) / (df[param].max() - df[param].min())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78pLPoWjtwdY"
      },
      "outputs": [],
      "source": [
        "criteria_weights = [1] * len(parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybzeJ7eYt0GZ"
      },
      "outputs": [],
      "source": [
        "# Compute weighted normalized matrix\n",
        "weighted_normalized_matrix = df_normalized.iloc[:, 1:] * criteria_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g0aAurst2qc"
      },
      "outputs": [],
      "source": [
        "# Compute positive and negative ideal solutions\n",
        "positive_ideal_solution = weighted_normalized_matrix.max(axis=0)\n",
        "negative_ideal_solution = weighted_normalized_matrix.min(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLM5tHvvt54-"
      },
      "outputs": [],
      "source": [
        "df_normalized[\"TOPSIS_Score\"] = np.linalg.norm(weighted_normalized_matrix - negative_ideal_solution, axis=1) / (\n",
        "        np.linalg.norm(weighted_normalized_matrix - positive_ideal_solution, axis=1) +\n",
        "        np.linalg.norm(weighted_normalized_matrix - negative_ideal_solution, axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcI1L2_Wt--A"
      },
      "outputs": [],
      "source": [
        "df_ranked = df_normalized.sort_values(by=\"TOPSIS_Score\", ascending=False).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xNIxq9zuBha"
      },
      "outputs": [],
      "source": [
        "df_ranked[\"Rank\"] = df_ranked.index + 1  # Rank starts from 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k68ismcVuGpI"
      },
      "outputs": [],
      "source": [
        "df_ranked.to_csv(\"topsis_results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn9XUovquJSf",
        "outputId": "92e94fb2-c018-46f0-be49-75e82bff8bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Rank                                          Model  TOPSIS_Score\n",
            "0      1                             facebook/bart-base      0.932619\n",
            "1      2                          deepset/sentence_bert      0.750122\n",
            "2      3                              bert-base-uncased      0.506996\n",
            "3      4                        distilbert-base-uncased      0.484476\n",
            "4      5                   allenai/longformer-base-4096      0.467023\n",
            "5      6                                   roberta-base      0.463823\n",
            "6      7             google/electra-small-discriminator      0.462087\n",
            "7      8                             vinai/phobert-base      0.452892\n",
            "8      9                           microsoft/mpnet-base      0.447087\n",
            "9     10                squeezebert/squeezebert-uncased      0.417772\n",
            "10    11  sentence-transformers/paraphrase-MiniLM-L6-v2      0.267881\n"
          ]
        }
      ],
      "source": [
        "print(df_ranked[[\"Rank\", \"Model\", \"TOPSIS_Score\"]])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}